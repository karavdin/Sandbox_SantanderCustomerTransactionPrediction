{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is taken from https://www.kaggle.com/c/santander-customer-transaction-prediction\n",
    "\n",
    "In this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.\n",
    "\n",
    "The data is anonimyzed, each row containing 200 numerical values identified just with a number.\n",
    "\n",
    "In this notebook we define ML models for testing their performance in training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "from keras.constraints import max_norm\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# Add RUC metric to monitor NN\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.constraints import max_norm\n",
    "def build_NN_128(X_train_NN):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 128, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                    kernel_regularizer=regularizers.l2(0.001), kernel_constraint = max_norm(5.),\n",
    "                   activity_regularizer=regularizers.l2(1e-7)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units = 32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.constraints import max_norm\n",
    "def build_NN_64(X_train_NN):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 64, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                    kernel_regularizer=regularizers.l2(0.0001), kernel_constraint = max_norm(5.),\n",
    "                   activity_regularizer=regularizers.l2(1e-6)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(units = 32, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.001), kernel_constraint=max_norm(5)))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_regularizer: Regularizer function applied to the kernel weights matrix\n",
    "#activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\")\n",
    "def build_NN_32(X_train_NN):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 32, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                    kernel_regularizer=regularizers.l2(3e-3), kernel_constraint = max_norm(5.),\n",
    "                   activity_regularizer=regularizers.l2(1e-6)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 16, activation='relu', kernel_regularizer=regularizers.l2(3e-3), \n",
    "                    kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-6)))\n",
    " #   model.add(Dropout(rate=0.1))\n",
    " #   model.add(Dense(units = 8, activation='relu', kernel_regularizer=regularizers.l2(3e-3), \n",
    " #                   kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-6)))\n",
    " #   model.add(Dropout(rate=0.1))\n",
    " #   model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(3e-3), \n",
    " #                   kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-6)))\n",
    " #   model.add(Dropout(rate=0.1))\n",
    " #   model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(3e-3), \n",
    " #                   kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-6)))\n",
    " #   model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "    #model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy',auc])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy',auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN_16(X_train_NN):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 16, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                    kernel_regularizer=regularizers.l2(0.005), kernel_constraint = max_norm(3.)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 8, activation='relu', kernel_regularizer=regularizers.l2(0.01), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.01), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(4, activation='tanh', kernel_regularizer=regularizers.l2(0.01), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN_8(X_train_NN):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 128, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                    kernel_regularizer=regularizers.l2(0.0001), kernel_constraint = max_norm(3.)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 8, activation='relu', kernel_regularizer=regularizers.l2(0.0001), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.0001), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.0001), kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units = 1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN_test_wrap(X_train_NN):\n",
    "    def bm():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 128, activation = \"relu\", input_dim = X_train_NN.shape[1], kernel_initializer = \"normal\", \n",
    "                        kernel_regularizer=regularizers.l2(0), kernel_constraint = max_norm(3.),\n",
    "                       activity_regularizer=regularizers.l2(0)))\n",
    "        model.add(Dropout(rate=0.1))\n",
    "        model.add(Dense(units = 64, activation='relu', kernel_regularizer=regularizers.l2(1e-3), \n",
    "                        kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-3)))\n",
    "        model.add(Dense(units = 1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "        model.summary()\n",
    "        return model\n",
    "    return bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "def init_rand_uni(n): #good for tanh, sigmoid, etc\n",
    "    scale = math.sqrt(1/n)\n",
    "    init_rand_uni = tf.initializers.random_uniform(minval=(-1)*scale,maxval=scale)\n",
    "    return init_rand_uni\n",
    "def init_rand_std(n): #good for ReLU\n",
    "    scale = math.sqrt(2/n)\n",
    "    init_rand_std = tf.initializers.random_normal(mean=0,stddev=scale)\n",
    "    return init_rand_std\n",
    "    \n",
    "def build_NN_test(X_train_NN):\n",
    "    adam=Adam(lr=1e-2)\n",
    "    #init_rand_uni=tf.initializers.random_uniform(minval=-1,maxval=1)\n",
    "    init_const=tf.constant_initializer(value=-1.00)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 64, activation = 'relu', input_dim = X_train_NN.shape[1], \n",
    "                    kernel_initializer = init_rand_std(X_train_NN.shape[1]), bias_initializer=init_rand_uni(X_train_NN.shape[1]),\n",
    "                    kernel_regularizer=regularizers.l2(5e-7), kernel_constraint = max_norm(5),\n",
    "                    activity_regularizer=regularizers.l2(1e-5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(units = 64, activation='relu', kernel_regularizer=regularizers.l2(5e-7),\n",
    "                    kernel_initializer = init_rand_std(64), bias_initializer=init_rand_uni(64),\n",
    "                    kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-5)))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(rate=0.2))  \n",
    "    #model.add(Dense(units = 8, activation='relu', kernel_regularizer=regularizers.l2(5e-7),\n",
    "    #                kernel_initializer = 'normal', bias_initializer=tf.constant_initializer(value=0.00),\n",
    "    #                kernel_constraint=max_norm(5),activity_regularizer=regularizers.l2(1e-6)))\n",
    "    model.add(Dense(units = 1, activation='sigmoid',kernel_initializer = init_rand_uni(64), bias_initializer=init_rand_uni(64)))\n",
    "    #model.add(Dense(units = 1, activation='sigmoid',kernel_initializer = 'normal', bias_initializer=tf.constant_initializer(value=0.50),))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc])\n",
    "    #model.compile(loss='mean_squared_logarithmic_error', optimizer=adam, metrics=['accuracy',auc])\n",
    "    #model.compile(loss='mean_squared_logarithmic_error', optimizer=adam, metrics=[auc])\n",
    "    #model.compile(loss='categorical_hinge', optimizer=adam, metrics=['accuracy',auc])\n",
    "    #model.compile(loss='hinge', optimizer=adam, metrics=['accuracy',auc])\n",
    "    #model.compile(loss='squared_hinge', optimizer=adam, metrics=['accuracy',auc])\n",
    "    \n",
    "    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy',auc])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def build_BalancedBaggingClassifier():\n",
    "    base_estimator = AdaBoostClassifier(n_estimators=200) \n",
    "    bbc = RUSBoostClassifier(n_estimators=200,base_estimator=base_estimator)\n",
    "    #base_estimator = AdaBoostClassifier(base_estimator=GradientBoostingClassifier(max_depth=5, learning_rate=0.9),n_estimators=20)\n",
    "    ###bbc = RUSBoostClassifier(n_estimators=200,base_estimator=base_estimator)\n",
    "    #base_estimator = DecisionTreeClassifier(max_depth = 100)\n",
    "    #bbc = BalancedBaggingClassifier(n_estimators=10,base_estimator=base_estimator, random_state=42, max_samples = 0.5)\n",
    "    return bbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
